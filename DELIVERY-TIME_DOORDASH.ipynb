{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**0. Preparation of the environment**"
      ],
      "metadata": {
        "id": "EIlfYc9Uvlw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aQeClGg6_91m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirmamos la ruta donde está el archivo csv\n",
        "%ls /content/drive/MyDrive/Colab_Notebooks/GITHUB/DOORDASH/historical_data.csv"
      ],
      "metadata": {
        "id": "FhwkPqE5uKX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yV7Mt_7492r"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Cargamos el archivo csv en un DataFrame\n",
        "delivery_data = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/Colab_Notebooks/GITHUB/DOORDASH/historical_data.csv\", encoding='utf-8', encoding_errors='replace')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Data description**"
      ],
      "metadata": {
        "id": "2ZehAdbrv7KN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Time features**\n",
        "\n",
        "* market_id: A city/region in which DoorDash operates, e.g., Los Angeles, given in the data as an id\n",
        "* created_at: Timestamp in UTC when the order was submitted by the consumer to DoorDash. (Note this timestamp is in UTC, but in case you need it, the actual timezone of the region was US/Pacific)\n",
        "* actual_delivery_time: Timestamp in UTC when the order was delivered to the consumer\n",
        "\n",
        "**Store features**\n",
        "\n",
        "* store_id: an id representing the restaurant the order was submitted for\n",
        "*store_primary_category: cuisine category of the restaurant, e.g., italian, asian\n",
        "*order_protocol: a store can receive orders from DoorDash through many modes. This field represents an id denoting the protocol\n",
        "\n",
        "**Order features**\n",
        "\n",
        "* total_items: total number of items in the order\n",
        "* subtotal: total value of the order submitted (in cents)\n",
        "* num_distinct_items: number of distinct items included in the order\n",
        "* min_item_price: price of the item with the least cost in the order (in cents)\n",
        "* max_item_price: price of the item with the highest cost in the order (in cents)\n",
        "\n",
        "**Market features**\n",
        "\n",
        "DoorDash being a marketplace, we have information on the state of marketplace when the order is placed, that can be used to estimate delivery time. The following features are values at the time of created_at (order submission time):\n",
        "\n",
        "*total_onshift_dashers: Number of available dashers who are within 10 miles of the store at the time of order creation\n",
        "*total_busy_dashers: Subset of above total_onshift_dashers who are currently working on an order\n",
        "*total_outstanding_orders: Number of orders within 10 miles of this order that are currently being processed.\n",
        "\n",
        "**Predictions from other models**\n",
        "\n",
        "We have predictions from other models for various stages of delivery process that we can use:\n",
        "\n",
        "* estimated_order_place_duration: Estimated time for the restaurant to receive the order from DoorDash (in seconds)\n",
        "* estimated_store_to_consumer_driving_duration: Estimated travel time between store and consumer (in seconds)"
      ],
      "metadata": {
        "id": "8wBUg-5gvQvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a consumer places an order on DoorDash, we show the expected time of delivery. It is very important for DoorDash to get this right, as it has a big impact on consumer experience.\n",
        "\n",
        "In this project we are going to predict the total delivery duration (in seconds) based on the moment the consumer submits de order (created_at) and when the order will be delivered to the consumer (actual_delivery_time)"
      ],
      "metadata": {
        "id": "6RWGwp85x_F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Data Exploration**"
      ],
      "metadata": {
        "id": "nCzOjMo12fhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_data.head()"
      ],
      "metadata": {
        "id": "z80e6DmwzDqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_data.info()"
      ],
      "metadata": {
        "id": "vFH0MIh80RZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the target variable for regression\n",
        "from datetime import datetime\n",
        "delivery_data[\"created_at\"] = pd.to_datetime(delivery_data['created_at'])\n",
        "delivery_data[\"actual_delivery_time\"] = pd.to_datetime(delivery_data['actual_delivery_time'])\n",
        "delivery_data[\"actual_total_delivery_duration\"] = (delivery_data[\"actual_delivery_time\"] - delivery_data[\"created_at\"]).dt.total_seconds()\n",
        "delivery_data.head()"
      ],
      "metadata": {
        "id": "F9EKJCBK0YPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new features\n",
        "# We can merge the estimated time of the restaurante to receive an order and the estimated time that takes from the store to deliver to the consumer as the non preparation duration\n",
        "delivery_data['estimated_non_prep_duration'] = delivery_data[\"estimated_store_to_consumer_driving_duration\"] + delivery_data[\"estimated_order_place_duration\"]\n",
        "# Ratio of available dashers at the time of order creation\n",
        "delivery_data[\"busy_dashers_ratio\"] = delivery_data[\"total_busy_dashers\"] / delivery_data[\"total_onshift_dashers\"]\n",
        "delivery_data.head()"
      ],
      "metadata": {
        "id": "46PW5Ghd1G4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.1. Managing nulls**"
      ],
      "metadata": {
        "id": "yclLEDYpkK8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NaN per column\\n: \", delivery_data.isna().sum())"
      ],
      "metadata": {
        "id": "mOqdQGoZm4DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dictionary with most frequent category of each store to fill null\n",
        "store_id_unique = delivery_data[\"store_id\"].unique().tolist()\n",
        "store_id_and_category = {store_id: delivery_data[delivery_data.store_id == store_id].store_primary_category.mode()\n",
        "                         for store_id in store_id_unique}"
      ],
      "metadata": {
        "id": "pCXS-NuK199N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill(store_id):\n",
        "    \"\"\"Return mode store category from the dictionary\"\"\"\n",
        "    try:\n",
        "        return store_id_and_category[store_id].values[0]\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# fill null values\n",
        "delivery_data[\"nan_free_store_primary_category\"] = delivery_data.store_id.apply(fill)"
      ],
      "metadata": {
        "id": "hixbUIj51_32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.2. Encoding**"
      ],
      "metadata": {
        "id": "AIX7e5FCkHKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if it makes sense to onehot encode nominal data columns\n",
        "print(delivery_data['market_id'].nunique())\n",
        "print(delivery_data['store_id'].nunique())\n",
        "print(delivery_data['nan_free_store_primary_category'].nunique())\n",
        "print(delivery_data['order_protocol'].nunique())"
      ],
      "metadata": {
        "id": "GhcKynICkTlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummies for market_id, order_protocol and nan_free_store_primary_category\n",
        "delivery_data = pd.get_dummies(delivery_data,\n",
        "                            columns=['market_id', 'order_protocol', 'nan_free_store_primary_category'],\n",
        "                            prefix=['market_id', 'order_protocol', 'category']\n",
        "                            )\n",
        "print(delivery_data.columns)"
      ],
      "metadata": {
        "id": "z1nTTplalUcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop unnecessary columns\n",
        "train_df = delivery_data.drop(columns = [\"created_at\", \"store_id\", \"store_primary_category\", \"actual_delivery_time\"])\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "R0AS-JzumvSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "id": "qQ3r8NTeo1NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace inf values with nan\n",
        "train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "# drop all nans\n",
        "train_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "xlkb1M0isB3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "id": "LygQinMg0zdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.3. Collinearity**"
      ],
      "metadata": {
        "id": "2rZiwkzetQ4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "V8BGeTLPw03A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# align dtype over dataset\n",
        "train_df = train_df.astype(\"float32\")"
      ],
      "metadata": {
        "id": "xywLnKZjw5s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "4mE8EFiktYug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 100 columns in our final dataset, which means there might be redundant features and potential collinearity which doesn't add any new knowledge to ML models. We start with the correlation matrix."
      ],
      "metadata": {
        "id": "78_0hHdBtX79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a mask for the upper triangle because the other traingle is its summetry. This way we have a better visualization\n",
        "correlation_matrix = train_df.corr()\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "\n",
        "# Drawing the heatmap with the mask\n",
        "plt.figure(figsize = (11,9))\n",
        "sns.heatmap(correlation_matrix, mask=mask, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, cmap=\"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q3KIqmu2unKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In the correlation matrix looks like there is an issue in category_indonesian\n",
        "train_df['category_indonesian'].describe()"
      ],
      "metadata": {
        "id": "-cPp4vnawUUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, category_indonesian has a lot of zeros as value, so we can drop this feature beacuse it has no effect. In order to drop redundant values and find the top correlated features we are going to use two functions. This way we can avoid multicollinearity by removing highly correlated features"
      ],
      "metadata": {
        "id": "NVVSJGMLx6RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_redundant_pairs(df):\n",
        "    \"\"\"Get diagonal and lower triangular pairs of correlation matrix\"\"\"\n",
        "    pairs_to_drop = set()\n",
        "    cols = df.columns\n",
        "    for i in range(0, df.shape[1]):\n",
        "        for j in range(0, i+1):\n",
        "            pairs_to_drop.add((cols[i], cols[j]))\n",
        "    return pairs_to_drop\n",
        "\n",
        "def get_top_abs_correlations(df, n=5):\n",
        "    \"\"\"Sort correlations in the descending order and return n highest results\"\"\"\n",
        "    au_corr = df.corr().abs().unstack()\n",
        "    labels_to_drop = get_redundant_pairs(df)\n",
        "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
        "    return au_corr[0:n]\n",
        "\n",
        "print(\"Top Absolute Correlations\")\n",
        "print(get_top_abs_correlations(train_df, 20))"
      ],
      "metadata": {
        "id": "jWoS6cUKxZn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop highly correlated features\n",
        "train_df = train_df.drop(columns=[\"total_onshift_dashers\", \"total_busy_dashers\", \"category_indonesian\",\n",
        "                                  \"estimated_non_prep_duration\", \"market_id_1.0\", \"market_id_2.0\",\n",
        "                                  \"market_id_3.0\", \"market_id_4.0\", \"market_id_5.0\", \"market_id_6.0\"])"
      ],
      "metadata": {
        "id": "Qz2LHfYdzfV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top Absolute Correlations\")\n",
        "print(get_top_abs_correlations(train_df, 20))"
      ],
      "metadata": {
        "id": "_RUphMgs0qhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(columns=[\"order_protocol_1.0\", \"order_protocol_2.0\", \"order_protocol_3.0\", \"order_protocol_4.0\",\n",
        "                                  \"order_protocol_5.0\", \"order_protocol_6.0\", \"order_protocol_7.0\"])"
      ],
      "metadata": {
        "id": "ACtXCxGyBijC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top Absolute Correlations\")\n",
        "print(get_top_abs_correlations(train_df, 20))"
      ],
      "metadata": {
        "id": "IQvQw-K3B2-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total item number or distinct items could affect the duration of the preparation process. Therefore, we do not prefer to drop them. Instead, we can use the power of feature engineering. We will create new columns to infer the contribution of these columns."
      ],
      "metadata": {
        "id": "3oS37-PxCBtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new features\n",
        "train_df[\"percent_distinct_item_of_total\"] = train_df[\"num_distinct_items\"] / train_df[\"total_items\"]\n",
        "train_df[\"avg_price_per_item\"] = train_df[\"subtotal\"] / train_df[\"total_items\"]\n",
        "train_df.drop(columns=[\"num_distinct_items\", \"subtotal\"], inplace=True)\n",
        "print(\"Top Absolute Correlations\")\n",
        "print(get_top_abs_correlations(train_df, 20))"
      ],
      "metadata": {
        "id": "3OeXQ_vzB87d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"price_range_of_items\"] = train_df[\"max_item_price\"] - train_df[\"min_item_price\"]\n",
        "train_df.drop(columns=[\"max_item_price\", \"min_item_price\"], inplace=True)\n",
        "print(\"Top Absolute Correlations\")\n",
        "print(get_top_abs_correlations(train_df, 20))"
      ],
      "metadata": {
        "id": "Hdw2WSJOCe43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "AmdylTRzCkt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature engineering has improved the dataset and now we have 82 features with less than 0.5 collinearity between them"
      ],
      "metadata": {
        "id": "rjG1ODdPDhmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.4. Multicollinearity**\n"
      ],
      "metadata": {
        "id": "GRIYokmkC5-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid having multiple variables correlated with each other we are going to use VIF which tells us the score by measuring how much our regression analysis is affected by collinearity. We will remove the features that have a vif score of over 20. This way it will be easier to interpret the model and reduce the risk of overffiting"
      ],
      "metadata": {
        "id": "yBq1xlE1D6tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def compute_vif(features):\n",
        "    \"\"\"Compute VIF score using variance_inflation_factor() function\"\"\"\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = features\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(train_df[features].values, i) for i in range(len(features))]\n",
        "    return vif_data.sort_values(by=['VIF']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "aP8uaHDeEaxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply VIF to all columns\n",
        "features = train_df.drop(columns=[\"actual_total_delivery_duration\"]).columns.to_list()\n",
        "vif_data = compute_vif(features)\n",
        "vif_data"
      ],
      "metadata": {
        "id": "RdIZMcXfGGZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop features with the highest VIF score until all VIF scores are below 20\n",
        "while vif_data['VIF'].max() > 20:\n",
        "    highest_vif_feature = vif_data.loc[vif_data['VIF'].idxmax(), 'feature']  # Get the feature with the highest VIF\n",
        "    print(f\"Removing {highest_vif_feature} due to high VIF\")\n",
        "\n",
        "    features.remove(highest_vif_feature)  # Remove the feature\n",
        "    vif_data = compute_vif(features)  # Recalculate VIF\n",
        "\n",
        "# Store the final selected features\n",
        "selected_features = vif_data['feature'].tolist()"
      ],
      "metadata": {
        "id": "3wX8B5G1Iu8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.5. Feature selection**\n"
      ],
      "metadata": {
        "id": "yars_V1uKx8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_df[selected_features]\n",
        "y = train_df[\"actual_total_delivery_duration\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5irq9uwpK1Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = [f\"feature {i}\" for i in range((X.shape[1]))]\n",
        "forest = RandomForestRegressor(random_state=42)\n",
        "forest.fit(X_train, y_train)\n",
        "feats = {} # a dict to hold feature_name: feature_importance\n",
        "for feature, importance in zip(X.columns, forest.feature_importances_):\n",
        "    feats[feature] = importance #add the name/value pair\n",
        "\n",
        "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
        "importances.sort_values(by='Gini-importance').plot(kind='bar', rot=90, figsize=(15,12))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cjazDke8NEFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the most important ones\n",
        "importances.sort_values(by='Gini-importance')[-35:].plot(kind='bar', rot=90, figsize=(15,12))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LQISSd5qNI2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply PCA to see feature contributions\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_Train=X_train.values\n",
        "X_Train=np.asarray(X_Train)\n",
        "\n",
        "# Finding normalised array of X_Train\n",
        "X_std=StandardScaler().fit_transform(X_Train)\n",
        "pca = PCA().fit(X_std)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlim(0,81)\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uR_zaiv5Mk1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA shows that we need to use at least 60 representative features to explain 80% of the dataset, which makes the PCA transformation useless since we already have 80 and could select the most important ones based on feature importance. However, if PCA would tell us it can explain the majority of variance with around 10 features - high reduction - we would continue with it"
      ],
      "metadata": {
        "id": "sZ3KigbsM_HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.Machine Learning**"
      ],
      "metadata": {
        "id": "i-M2euRYOhIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.1. Classic machine learning regression**"
      ],
      "metadata": {
        "id": "ezu81-u46-Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "6AxLhzwXMknV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def calculate_rmse(y_test, y_pred, model_name):\n",
        "    \"\"\"Calculate and print RMSE for a regression model\"\"\"\n",
        "\n",
        "    # Compute RMSE (Root Mean Squared Error)\n",
        "    rmse_error = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Print the result\n",
        "    print(f\"Error (RMSE) = {rmse_error:.4f} in {model_name}\")\n",
        "\n",
        "    return rmse_error"
      ],
      "metadata": {
        "id": "BHF3UzT3SOzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Root Mean Squared Error to measure error based on the sensitivity of RMSE for high error terms. In our thought, the consumer patience with delaying delivery could decrease exponentially with time."
      ],
      "metadata": {
        "id": "m2cmjA4VSXcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "5ZlMAwV-Vf_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize prediction dictionary to store RMSE results\n",
        "pred_dict = {}\n",
        "\n",
        "# Define models and settings\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"SVM Regression\": SVR(kernel=\"rbf\"),\n",
        "    \"XGBoost\": XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, random_state=42),\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    pred_dict[name] = calculate_rmse(y_test, y_pred, name)\n",
        "\n",
        "# Polynomial Regression with different degrees\n",
        "poly_degrees = [2]\n",
        "\n",
        "for degree in poly_degrees:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
        "    X_test_poly = poly.transform(X_test_scaled)\n",
        "\n",
        "    poly_model = LinearRegression()\n",
        "    poly_model.fit(X_train_poly, y_train)\n",
        "    y_pred_poly = poly_model.predict(X_test_poly)\n",
        "\n",
        "    model_name = f\"Polynomial Regression (Degree {degree})\"\n",
        "    pred_dict[model_name] = calculate_rmse(y_test, y_pred_poly, model_name)"
      ],
      "metadata": {
        "id": "zR2M0LJcwjaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.2. Deep Learning**"
      ],
      "metadata": {
        "id": "5DK_bZ3061qi"
      }
    }
  ]
}